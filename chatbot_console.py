import subprocess

def run_ollama(prompt):
    """
    Envoie le prompt √† Ollama (mod√®le Mistral)
    et g√®re les erreurs d'encodage.
    """
    try:
        r = subprocess.run(
            ["ollama", "run", "mistral", prompt],
            capture_output=True,
            text=True,
            encoding="utf-8",
            errors="ignore"
        )

        if r.returncode != 0:
            return f"‚ùå Erreur Ollama : {r.stderr.strip()}"
        return r.stdout.strip() if r.stdout else "‚ö†Ô∏è Pas de r√©ponse du mod√®le."
    except Exception as e:
        return f"‚ùå Erreur Python : {e}"


print("ü§ñ Chatbot Mistral (Lab 5)")
print("Tape 'quit' pour arr√™ter.\n")

while True:
    q = input("Vous : ")
    if q.lower() in ["quit", "exit"]:
        print("Agent : √Ä bient√¥t üëã")
        break

    # Simule la temp√©rature pour ton analyse (affichage seulement)
    if "cr√©atif" in q.lower() or "po√®me" in q.lower() or "histoire" in q.lower():
        temp = 0.9
    elif "r√©sume" in q.lower() or "d√©finis" in q.lower() or "explique" in q.lower():
        temp = 0.3
    else:
        temp = 0.7

    print(f"[Temp√©rature simul√©e : {temp}]")
    reponse = run_ollama(q)
    print("Agent :", reponse, "\n")
